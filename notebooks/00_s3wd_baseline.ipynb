{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4db9d230",
   "metadata": {},
   "source": [
    "# 00 · S3WD Baseline（中文）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "995b9506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【可视化字体】已设置为宋体优先（若系统缺失则自动回退）。\n",
      "【Python】 3.11.5\n",
      "【Pandas/Numpy】 2.0.3 1.26.4\n",
      "【模块路径】\n",
      "  zh_utils   -> e:\\yan\\组\\三支决策\\机器学习\\C三支决策与不平衡数据集分类\\S3WD实验\\s3wdlib\\zh_utils.py\n",
      "  data_io    -> e:\\yan\\组\\三支决策\\机器学习\\C三支决策与不平衡数据集分类\\S3WD实验\\s3wdlib\\data_io.py\n",
      "  features   -> e:\\yan\\组\\三支决策\\机器学习\\C三支决策与不平衡数据集分类\\S3WD实验\\s3wdlib\\features.py\n",
      "  kwb        -> e:\\yan\\组\\三支决策\\机器学习\\C三支决策与不平衡数据集分类\\S3WD实验\\s3wdlib\\kwb.py\n",
      "  objective  -> e:\\yan\\组\\三支决策\\机器学习\\C三支决策与不平衡数据集分类\\S3WD实验\\s3wdlib\\objective.py\n",
      "  trainer    -> e:\\yan\\组\\三支决策\\机器学习\\C三支决策与不平衡数据集分类\\S3WD实验\\s3wdlib\\trainer.py\n",
      "【函数签名】load_table_auto: (path: 'str', label_col: 'Optional[str | int]' = None, positive_label=1, continuous_label: 'Optional[str]' = None, threshold: 'Optional[float]' = None, threshold_op: 'str' = '>=') -> 'Tuple[pd.DataFrame, pd.Series]'\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 0. 环境初始化与模块导入（中文）\n",
    "# ================================\n",
    "import os, sys, platform, importlib, inspect, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (f1_score, balanced_accuracy_score, precision_score, recall_score,\n",
    "                             matthews_corrcoef, cohen_kappa_score, roc_auc_score)\n",
    "# 1) 确保项目根目录在 sys.path[0]（notebooks/ 的上一级）\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "# 2) 导入自研库（s3wdlib）各模块，并强制重载确保拿到最新版实现\n",
    "import s3wdlib.zh_utils as zh_utils\n",
    "import s3wdlib.data_io as data_io\n",
    "import s3wdlib.features as features\n",
    "import s3wdlib.kwb as kwb\n",
    "import s3wdlib.objective as objective\n",
    "import s3wdlib.trainer as trainer\n",
    "\n",
    "importlib.reload(zh_utils)\n",
    "importlib.reload(data_io)\n",
    "importlib.reload(features)\n",
    "importlib.reload(kwb)\n",
    "importlib.reload(objective)\n",
    "importlib.reload(trainer)\n",
    "\n",
    "# 3) 把常用符号直接引入（可读性更好）\n",
    "from s3wdlib.zh_utils import set_chinese_font, fix_minus\n",
    "from s3wdlib.data_io import load_table_auto, minmax_scale_fit_transform\n",
    "from s3wdlib.features import rank_features_mi, make_levels\n",
    "from s3wdlib.kwb import KWBProbEstimator\n",
    "from s3wdlib.objective import S3WDParams\n",
    "from s3wdlib.trainer import PSOParams, pso_learn_thresholds\n",
    "from s3wdlib.config_loader import load_yaml_cfg, extract_vars, show_cfg\n",
    "\n",
    "# 4) 可视化中文设置（宋体优先，负号正常）\n",
    "set_chinese_font(); fix_minus()\n",
    "print(\"【可视化字体】已设置为宋体优先（若系统缺失则自动回退）。\")\n",
    "\n",
    "# 5) 版本与路径自检（定位是否导入了正确文件）\n",
    "print(\"【Python】\", platform.python_version())\n",
    "print(\"【Pandas/Numpy】\", pd.__version__, np.__version__)\n",
    "print(\"【模块路径】\")\n",
    "print(\"  zh_utils   ->\", zh_utils.__file__)\n",
    "print(\"  data_io    ->\", data_io.__file__)\n",
    "print(\"  features   ->\", features.__file__)\n",
    "print(\"  kwb        ->\", kwb.__file__)\n",
    "print(\"  objective  ->\", objective.__file__)\n",
    "print(\"  trainer    ->\", trainer.__file__)\n",
    "print(\"【函数签名】load_table_auto:\", inspect.signature(data_io.load_table_auto))\n",
    "\n",
    "# 6) 随机种子（方便复现；如需完全一致可统一设置）\n",
    "np.random.seed(42)\n",
    "\n",
    "# 7) 警告精简（可选）\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d3cdf5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【配置快照】\n",
      "- DATA: {'data_dir': '../data', 'data_file': 'australian_credit.csv', 'continuous_label': None, 'threshold': None, 'threshold_op': None, 'label_col': 'class', 'positive_label': 1, 'test_size': 0.3, 'val_size': 0.3, 'random_state': 42}\n",
      "- LEVEL: {'level_pcts': [0.6, 0.8, 1.0], 'ranker': 'mi'}\n",
      "- KWB: {'k': 6, 'metric': 'euclidean', 'eps': 1e-06}\n",
      "- S3WD: {'c1': 0.37, 'c2': 0.63, 'xi_min': 0.1, 'theta_pos': 2.0, 'theta_neg': 1.0, 'penalty_large': 1000000.0, 'gamma_last': True, 'gap': 0.02}\n",
      "- PSO: {'particles': 20, 'iters': 60, 'w_max': 0.9, 'w_min': 0.4, 'c1': 2.8, 'c2': 1.3, 'seed': 42}\n",
      "【参数就绪（来自 YAML）】 {'DATA_PATH': '../data\\\\australian_credit.csv', 'label': 'class==1', 'splits': 'test=0.3, val=0.3, seed=42', 'kwb.k': 6, 'pso': {'particles': 20, 'iters': 60}}\n"
     ]
    }
   ],
   "source": [
    "# === 配置接入（YAML → dataclass → 变量字典）===\n",
    "# wine\n",
    "# CFG = load_yaml_cfg(\"../configs/s3wd_wine.yaml\")  # ← 如换配置文件，只改这里\n",
    "# Heart\n",
    "# CFG = load_yaml_cfg(\"../configs/s3wd_heart.yaml\")\n",
    "# Credit\n",
    "CFG = load_yaml_cfg(\"../configs/s3wd_credit.yaml\")\n",
    "\n",
    "V   = extract_vars(CFG)\n",
    "show_cfg(CFG)\n",
    "\n",
    "# ✅ 兼容“连续列二值化”和“已有二值标签”两种配置\n",
    "if \"CONT_LABEL\" in V:\n",
    "    label_desc = f\"{V['CONT_LABEL']}{V['CONT_OP']}{V['CONT_THRESH']}\"\n",
    "elif \"LABEL_COL\" in V:\n",
    "    label_desc = f\"{V['LABEL_COL']}=={V.get('POSITIVE_LABEL', 1)}\"\n",
    "else:\n",
    "    label_desc = \"(未检测到标签配置)\"\n",
    "\n",
    "print(\"【参数就绪（来自 YAML）】\", {\n",
    "    \"DATA_PATH\": V[\"DATA_PATH\"],\n",
    "    \"label\": label_desc,\n",
    "    \"splits\": f\"test={V['TEST_SIZE']}, val={V['VAL_SIZE']}, seed={V['SEED']}\",\n",
    "    \"kwb.k\": V[\"KWB_K\"],\n",
    "    \"pso\": {\"particles\": V[\"PSO_particles\"], \"iters\": V[\"PSO_iters\"]}\n",
    "})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "41be3dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【标签策略】已有标签列：class == 1 视为正类\n",
      "【数据加载完毕】样本数=690，特征数=14，正类比例=0.4449\n",
      "【数据加载】X_all, y_all = (690, 14) (690,)\n",
      "【数据切分】Xtr/Xte = (483, 14) (207, 14)\n"
     ]
    }
   ],
   "source": [
    "# === 读取数据（兼容“连续列二值化 / 已有二值标签”两种配置）+ 切分 ===\n",
    "from s3wdlib.data_io import load_table_auto\n",
    "\n",
    "# 统一参数打包（不存在的键用 .get() 不报错）\n",
    "kw = dict(\n",
    "    path=V[\"DATA_PATH\"],\n",
    "    label_col=V.get(\"LABEL_COL\"),\n",
    "    positive_label=V.get(\"POSITIVE_LABEL\"),\n",
    "    continuous_label=V.get(\"CONT_LABEL\"),\n",
    "    threshold=V.get(\"CONT_THRESH\"),\n",
    "    threshold_op=V.get(\"CONT_OP\"),\n",
    ")\n",
    "\n",
    "# 友好提示\n",
    "if V.get(\"CONT_LABEL\") is not None:\n",
    "    print(f\"【标签策略】连续列二值化：{V['CONT_LABEL']} {V['CONT_OP']} {V['CONT_THRESH']}\")\n",
    "elif V.get(\"LABEL_COL\") is not None:\n",
    "    print(f\"【标签策略】已有标签列：{V['LABEL_COL']} == {V.get('POSITIVE_LABEL', 1)} 视为正类\")\n",
    "else:\n",
    "    raise RuntimeError(\"未检测到标签配置（既无 CONT_* 也无 LABEL_COL）。请检查 YAML。\")\n",
    "\n",
    "# 读取数据\n",
    "X_all, y_all = load_table_auto(**kw)\n",
    "print(\"【数据加载】X_all, y_all =\", X_all.shape, y_all.shape)\n",
    "\n",
    "# 切分（与 YAML 一致）\n",
    "Xtr, Xte, ytr, yte = train_test_split(\n",
    "    X_all, y_all,\n",
    "    test_size=V[\"TEST_SIZE\"],\n",
    "    random_state=V[\"SEED\"],\n",
    "    stratify=y_all\n",
    ")\n",
    "print(\"【数据切分】Xtr/Xte =\", Xtr.shape, Xte.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9229e4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【归一化】已对训练/测试集进行 MinMax 缩放到 [0,1]。\n",
      "【分层复核】总特征=14 | L1=8 L2=11 L3=14\n"
     ]
    }
   ],
   "source": [
    "# 1) 训练集内切出验证集（仅用于阈值寻优）\n",
    "Xtr_sub, Xva, ytr_sub, yva = train_test_split(\n",
    "    Xtr, ytr, test_size=V[\"VAL_SIZE\"], stratify=ytr, random_state=V[\"SEED\"]\n",
    ")\n",
    "\n",
    "# 2) 归一化（仅在训练子集拟合）\n",
    "Xtr2, Xva2, scaler = minmax_scale_fit_transform(Xtr_sub, Xva)\n",
    "Xte2 = pd.DataFrame(scaler.transform(Xte), columns=Xte.columns)\n",
    "\n",
    "# 3) 分层（训练子集上）\n",
    "feat_rank, mi_vals = rank_features_mi(Xtr2, ytr_sub)\n",
    "L1, L2, L3 = make_levels(feat_rank)\n",
    "print(f\"【分层复核】总特征={len(feat_rank)} | L1={len(L1)} L2={len(L2)} L3={len(L3)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22732d74",
   "metadata": {},
   "source": [
    "## KWB（Algorithm 1）训练与三层概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "878a9ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【KWB 完成】验证/测试三层概率就绪。\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4) KWB（训练子集拟合；验证/测试上出概率）\n",
    "kwb1 = KWBProbEstimator(k=V[\"KWB_K\"]).fit(Xtr2[L1], ytr_sub)\n",
    "kwb2 = KWBProbEstimator(k=V[\"KWB_K\"]).fit(Xtr2[L2], ytr_sub)\n",
    "kwb3 = KWBProbEstimator(k=V[\"KWB_K\"]).fit(Xtr2[L3], ytr_sub)\n",
    "p1_va = kwb1.predict_proba(Xva2[L1]); p2_va = kwb2.predict_proba(Xva2[L2]); p3_va = kwb3.predict_proba(Xva2[L3])\n",
    "p1_te = kwb1.predict_proba(Xte2[L1]); p2_te = kwb2.predict_proba(Xte2[L2]); p3_te = kwb3.predict_proba(Xte2[L3])\n",
    "print(\"【KWB 完成】验证/测试三层概率就绪。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f8e4f8",
   "metadata": {},
   "source": [
    "## 验证集 PSO 学阈值（信息增益−后悔值 + 单调序 + ξ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "59e73fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【PSO 学到阈值（验证集）】 ['α1=0.9973/β1=0.3012', 'α2=0.9152/β2=0.3012', 'α3=0.6614/β3=0.3341'] γ3=0.5000\n",
      "【适应度/约束】 {'fit': -0.1878, 'pen_bnd': 0.0, 'pen_mono': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# 5) 验证集 PSO 学阈值（信息增益−后悔值 + 单调序 + ξ）\n",
    "s3 = S3WDParams(\n",
    "    c1=V[\"S3_c1\"], c2=V[\"S3_c2\"], xi_min=V[\"S3_xi_min\"],\n",
    "    theta_pos=V[\"S3_theta_pos\"], theta_neg=V[\"S3_theta_neg\"],\n",
    "    penalty_large=V[\"S3_penalty_large\"],\n",
    "    gamma_last=V.get(\"S3_gamma_last\"),   # ← 用 gamma_last（True 或 0.5）\n",
    "    gap=V.get(\"S3_gap\", 0.02)\n",
    ")\n",
    "pso = PSOParams(\n",
    "    particles=V[\"PSO_particles\"], iters=V[\"PSO_iters\"],\n",
    "    w_max=V[\"PSO_w_max\"], w_min=V[\"PSO_w_min\"],\n",
    "    c1=V[\"PSO_c1\"], c2=V[\"PSO_c2\"], seed=V[\"PSO_seed\"]\n",
    ")\n",
    "(best_th, best_fit, detail) = pso_learn_thresholds([p1_va, p2_va, p3_va], yva.values, s3, pso)\n",
    "\n",
    "alphas, betas, gamma3 = best_th\n",
    "print(\"【PSO 学到阈值（验证集）】\", [f\"α{i+1}={a:.4f}/β{i+1}={b:.4f}\" for i,(a,b) in enumerate(zip(alphas,betas))], f\"γ3={gamma3:.4f}\")\n",
    "print(\"【适应度/约束】\", {\"fit\":round(best_fit,4), \"pen_bnd\":detail.get(\"pen_bnd\",None), \"pen_mono\":detail.get(\"pen_mono\",None)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769b5df6",
   "metadata": {},
   "source": [
    "## 测试集序贯三支决策 + 评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "53342cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【样本流转（学到的阈值）】L1 POS/BND/NEG = 0 117 90  | L2 POS/BND/NEG = 0 112 5  | L3 POS/NEG = 100 12\n",
      "【评估（测试集）】 {'F1': 0.8333, 'BAC': 0.8478, 'Prec': 0.8, 'Rec': 0.8696, 'MCC': 0.6917, 'Kappa': 0.6897, 'AUC': 0.8478}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 6) 测试集序贯三支决策 + 评估\n",
    "def _seq_predict_eval(p1, p2, p3, y_true, a1, b1, a2, b2, g3):\n",
    "    POS1 = (p1 >= a1); NEG1 = (p1 <= b1); BND1 = (~POS1) & (~NEG1)\n",
    "    p2s = p2[BND1]; POS2 = np.zeros_like(BND1, bool); NEG2 = np.zeros_like(BND1, bool)\n",
    "    POS2[BND1] = (p2s >= a2); NEG2[BND1] = (p2s <= b2)\n",
    "    BND2 = BND1 & (~POS2) & (~NEG2)\n",
    "    p3s = p3[BND2]; POS3 = np.zeros_like(BND2, bool); NEG3 = np.zeros_like(BND2, bool)\n",
    "    POS3[BND2] = (p3s >= g3); NEG3[BND2] = ~POS3[BND2]\n",
    "    y_hat = np.full_like(y_true, -1, int)\n",
    "    y_hat[POS1]=1; y_hat[NEG1]=0; y_hat[POS2]=1; y_hat[NEG2]=0; y_hat[POS3]=1; y_hat[NEG3]=0\n",
    "    flow = {\"L1\":(int(POS1.sum()), int(BND1.sum()), int(NEG1.sum())),\n",
    "            \"L2\":(int(POS2.sum()), int(BND2.sum()), int(NEG2.sum())),\n",
    "            \"L3\":(int(POS3.sum()), int(NEG3.sum()))}\n",
    "    return y_hat, flow\n",
    "\n",
    "a1,b1 = float(alphas[0]), float(betas[0])\n",
    "a2,b2 = float(alphas[1]), float(betas[1])\n",
    "g3    = float(gamma3)\n",
    "\n",
    "y_hat, flow = _seq_predict_eval(p1_te, p2_te, p3_te, yte.values,\n",
    "                           float(alphas[0]), float(betas[0]),\n",
    "                           float(alphas[1]), float(betas[1]),\n",
    "                           g3=0.5)\n",
    "print(\"【样本流转（学到的阈值）】L1 POS/BND/NEG =\", *flow[\"L1\"], \" | L2 POS/BND/NEG =\", *flow[\"L2\"], \" | L3 POS/NEG =\", *flow[\"L3\"])\n",
    "\n",
    "mask = (y_hat >= 0)\n",
    "metrics = {\n",
    "    'F1': round(f1_score(yte[mask], y_hat[mask]),4),\n",
    "    'BAC': round(balanced_accuracy_score(yte[mask], y_hat[mask]),4),\n",
    "    'Prec': round(precision_score(yte[mask], y_hat[mask]),4),\n",
    "    'Rec': round(recall_score(yte[mask], y_hat[mask]),4),\n",
    "    'MCC': round(matthews_corrcoef(yte[mask], y_hat[mask]),4),\n",
    "    'Kappa': round(cohen_kappa_score(yte[mask], y_hat[mask]),4),\n",
    "    'AUC': round(roc_auc_score(yte[mask], y_hat[mask]),4)\n",
    "}\n",
    "print(\"【评估（测试集）】\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ad80fd",
   "metadata": {},
   "source": [
    "## 10 次独立 70/30 划分（Train→Val(寻优)→Test(评估)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d3fafc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【数据加载完毕】样本数=690，特征数=14，正类比例=0.4449\n",
      "【归一化】已对训练/测试集进行 MinMax 缩放到 [0,1]。\n",
      "【数据加载完毕】样本数=690，特征数=14，正类比例=0.4449\n",
      "【归一化】已对训练/测试集进行 MinMax 缩放到 [0,1]。\n",
      "【数据加载完毕】样本数=690，特征数=14，正类比例=0.4449\n",
      "【归一化】已对训练/测试集进行 MinMax 缩放到 [0,1]。\n",
      "【数据加载完毕】样本数=690，特征数=14，正类比例=0.4449\n",
      "【归一化】已对训练/测试集进行 MinMax 缩放到 [0,1]。\n",
      "【数据加载完毕】样本数=690，特征数=14，正类比例=0.4449\n",
      "【归一化】已对训练/测试集进行 MinMax 缩放到 [0,1]。\n",
      "【数据加载完毕】样本数=690，特征数=14，正类比例=0.4449\n",
      "【归一化】已对训练/测试集进行 MinMax 缩放到 [0,1]。\n",
      "【数据加载完毕】样本数=690，特征数=14，正类比例=0.4449\n",
      "【归一化】已对训练/测试集进行 MinMax 缩放到 [0,1]。\n",
      "【数据加载完毕】样本数=690，特征数=14，正类比例=0.4449\n",
      "【归一化】已对训练/测试集进行 MinMax 缩放到 [0,1]。\n",
      "【数据加载完毕】样本数=690，特征数=14，正类比例=0.4449\n",
      "【归一化】已对训练/测试集进行 MinMax 缩放到 [0,1]。\n",
      "【数据加载完毕】样本数=690，特征数=14，正类比例=0.4449\n",
      "【归一化】已对训练/测试集进行 MinMax 缩放到 [0,1]。\n",
      "【10 次独立划分结果（前 5 行）】\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1</th>\n",
       "      <th>BAC</th>\n",
       "      <th>Prec</th>\n",
       "      <th>Rec</th>\n",
       "      <th>MCC</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>AUC</th>\n",
       "      <th>L1_POS</th>\n",
       "      <th>L1_BND</th>\n",
       "      <th>L1_NEG</th>\n",
       "      <th>...</th>\n",
       "      <th>L3_POS</th>\n",
       "      <th>L3_NEG</th>\n",
       "      <th>alpha1</th>\n",
       "      <th>beta1</th>\n",
       "      <th>alpha2</th>\n",
       "      <th>beta2</th>\n",
       "      <th>gamma3</th>\n",
       "      <th>fit</th>\n",
       "      <th>pen_bnd</th>\n",
       "      <th>pen_mono</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.8333</td>\n",
       "      <td>0.8478</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.8696</td>\n",
       "      <td>0.6917</td>\n",
       "      <td>0.6897</td>\n",
       "      <td>0.8478</td>\n",
       "      <td>0</td>\n",
       "      <td>117</td>\n",
       "      <td>90</td>\n",
       "      <td>...</td>\n",
       "      <td>100</td>\n",
       "      <td>12</td>\n",
       "      <td>0.9973</td>\n",
       "      <td>0.3012</td>\n",
       "      <td>0.9152</td>\n",
       "      <td>0.3012</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.1878</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.8367</td>\n",
       "      <td>0.8500</td>\n",
       "      <td>0.7885</td>\n",
       "      <td>0.8913</td>\n",
       "      <td>0.6957</td>\n",
       "      <td>0.6910</td>\n",
       "      <td>0.8500</td>\n",
       "      <td>0</td>\n",
       "      <td>110</td>\n",
       "      <td>97</td>\n",
       "      <td>...</td>\n",
       "      <td>104</td>\n",
       "      <td>4</td>\n",
       "      <td>0.8981</td>\n",
       "      <td>0.3459</td>\n",
       "      <td>0.8981</td>\n",
       "      <td>0.3459</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0926</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.8090</td>\n",
       "      <td>0.8304</td>\n",
       "      <td>0.8372</td>\n",
       "      <td>0.7826</td>\n",
       "      <td>0.6664</td>\n",
       "      <td>0.6652</td>\n",
       "      <td>0.8304</td>\n",
       "      <td>0</td>\n",
       "      <td>103</td>\n",
       "      <td>104</td>\n",
       "      <td>...</td>\n",
       "      <td>86</td>\n",
       "      <td>9</td>\n",
       "      <td>0.9972</td>\n",
       "      <td>0.2881</td>\n",
       "      <td>0.9972</td>\n",
       "      <td>0.3560</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0473</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.8061</td>\n",
       "      <td>0.8207</td>\n",
       "      <td>0.7596</td>\n",
       "      <td>0.8587</td>\n",
       "      <td>0.6373</td>\n",
       "      <td>0.6330</td>\n",
       "      <td>0.8207</td>\n",
       "      <td>0</td>\n",
       "      <td>113</td>\n",
       "      <td>94</td>\n",
       "      <td>...</td>\n",
       "      <td>104</td>\n",
       "      <td>6</td>\n",
       "      <td>0.9492</td>\n",
       "      <td>0.3535</td>\n",
       "      <td>0.9031</td>\n",
       "      <td>0.3535</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.8083</td>\n",
       "      <td>0.8239</td>\n",
       "      <td>0.7723</td>\n",
       "      <td>0.8478</td>\n",
       "      <td>0.6440</td>\n",
       "      <td>0.6416</td>\n",
       "      <td>0.8239</td>\n",
       "      <td>0</td>\n",
       "      <td>115</td>\n",
       "      <td>92</td>\n",
       "      <td>...</td>\n",
       "      <td>101</td>\n",
       "      <td>10</td>\n",
       "      <td>0.8892</td>\n",
       "      <td>0.2508</td>\n",
       "      <td>0.8892</td>\n",
       "      <td>0.2508</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.0591</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       F1     BAC    Prec     Rec     MCC   Kappa     AUC  L1_POS  L1_BND  \\\n",
       "0  0.8333  0.8478  0.8000  0.8696  0.6917  0.6897  0.8478       0     117   \n",
       "1  0.8367  0.8500  0.7885  0.8913  0.6957  0.6910  0.8500       0     110   \n",
       "2  0.8090  0.8304  0.8372  0.7826  0.6664  0.6652  0.8304       0     103   \n",
       "3  0.8061  0.8207  0.7596  0.8587  0.6373  0.6330  0.8207       0     113   \n",
       "4  0.8083  0.8239  0.7723  0.8478  0.6440  0.6416  0.8239       0     115   \n",
       "\n",
       "   L1_NEG  ...  L3_POS  L3_NEG  alpha1   beta1  alpha2   beta2  gamma3  \\\n",
       "0      90  ...     100      12  0.9973  0.3012  0.9152  0.3012     0.5   \n",
       "1      97  ...     104       4  0.8981  0.3459  0.8981  0.3459     0.5   \n",
       "2     104  ...      86       9  0.9972  0.2881  0.9972  0.3560     0.5   \n",
       "3      94  ...     104       6  0.9492  0.3535  0.9031  0.3535     0.5   \n",
       "4      92  ...     101      10  0.8892  0.2508  0.8892  0.2508     0.5   \n",
       "\n",
       "      fit  pen_bnd  pen_mono  \n",
       "0 -0.1878      0.0       0.0  \n",
       "1  0.0926      0.0       0.0  \n",
       "2  0.0473      0.0       0.0  \n",
       "3  0.1810      0.0       0.0  \n",
       "4 -0.0591      0.0       0.0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【均值 ± 标准差】\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F1</th>\n",
       "      <td>0.8335</td>\n",
       "      <td>0.0194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BAC</th>\n",
       "      <td>0.8480</td>\n",
       "      <td>0.0177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prec</th>\n",
       "      <td>0.7998</td>\n",
       "      <td>0.0255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rec</th>\n",
       "      <td>0.8717</td>\n",
       "      <td>0.0389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MCC</th>\n",
       "      <td>0.6931</td>\n",
       "      <td>0.0345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kappa</th>\n",
       "      <td>0.6898</td>\n",
       "      <td>0.0343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUC</th>\n",
       "      <td>0.8480</td>\n",
       "      <td>0.0177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         mean     std\n",
       "F1     0.8335  0.0194\n",
       "BAC    0.8480  0.0177\n",
       "Prec   0.7998  0.0255\n",
       "Rec    0.8717  0.0389\n",
       "MCC    0.6931  0.0345\n",
       "Kappa  0.6898  0.0343\n",
       "AUC    0.8480  0.0177"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === 外层评测：10 次独立 70/30 划分（Train→Val(寻优)→Test(评估)）===\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.metrics import (f1_score, balanced_accuracy_score, precision_score, recall_score,\n",
    "                             matthews_corrcoef, cohen_kappa_score, roc_auc_score)\n",
    "import numpy as np, pandas as pd, importlib\n",
    "\n",
    "# 确保拿到最新版目标/训练器实现\n",
    "import s3wdlib.objective as objective, s3wdlib.trainer as trainer\n",
    "importlib.reload(objective); importlib.reload(trainer)\n",
    "from s3wdlib.objective import S3WDParams\n",
    "from s3wdlib.trainer import PSOParams, pso_learn_thresholds\n",
    "from s3wdlib.data_io import minmax_scale_fit_transform, load_table_auto\n",
    "from s3wdlib.features import rank_features_mi, make_levels\n",
    "from s3wdlib.kwb import KWBProbEstimator\n",
    "\n",
    "def _seq_predict(p1, p2, p3, y_true, a1, b1, a2, b2, g3):\n",
    "    POS1 = (p1 >= a1); NEG1 = (p1 <= b1); BND1 = (~POS1) & (~NEG1)\n",
    "    p2s = p2[BND1]; POS2 = np.zeros_like(BND1, bool); NEG2 = np.zeros_like(BND1, bool)\n",
    "    POS2[BND1] = (p2s >= a2); NEG2[BND1] = (p2s <= b2)\n",
    "    BND2 = BND1 & (~POS2) & (~NEG2)\n",
    "    p3s = p3[BND2]; POS3 = np.zeros_like(BND2, bool); NEG3 = np.zeros_like(BND2, bool)\n",
    "    POS3[BND2] = (p3s >= g3); NEG3[BND2] = ~POS3[BND2]\n",
    "    y_hat = np.full_like(y_true, -1, int)\n",
    "    y_hat[POS1]=1; y_hat[NEG1]=0; y_hat[POS2]=1; y_hat[NEG2]=0; y_hat[POS3]=1; y_hat[NEG3]=0\n",
    "    flow = {\"L1\":(int(POS1.sum()), int(BND1.sum()), int(NEG1.sum())),\n",
    "            \"L2\":(int(POS2.sum()), int(BND2.sum()), int(NEG2.sum())),\n",
    "            \"L3\":(int(POS3.sum()), int(NEG3.sum()))}\n",
    "    return y_hat, flow\n",
    "\n",
    "def _safe_auc(y_true, y_pred):\n",
    "    try:\n",
    "        return roc_auc_score(y_true, y_pred)\n",
    "    except Exception:\n",
    "        return float(\"nan\")\n",
    "\n",
    "def run_one_split(seed: int):\n",
    "    # === 读取全量数据（兼容 “连续列二值化 / 已有标签列”）===\n",
    "    kw = dict(\n",
    "        path=V[\"DATA_PATH\"],\n",
    "        label_col=V.get(\"LABEL_COL\"),\n",
    "        positive_label=V.get(\"POSITIVE_LABEL\"),\n",
    "        continuous_label=V.get(\"CONT_LABEL\"),\n",
    "        threshold=V.get(\"CONT_THRESH\"),\n",
    "        threshold_op=V.get(\"CONT_OP\"),\n",
    "    )\n",
    "    X_all, y_all = load_table_auto(**kw)\n",
    "\n",
    "    # === 外层一次 70/30 划分 ===\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=V[\"TEST_SIZE\"], random_state=seed)\n",
    "    (tr_idx, te_idx), = sss.split(X_all, y_all)\n",
    "    Xtr_all, Xte = X_all.iloc[tr_idx], X_all.iloc[te_idx]\n",
    "    ytr_all, yte  = y_all.iloc[tr_idx], y_all.iloc[te_idx]\n",
    "\n",
    "    # === 训练集再切 val（仅用于阈值寻优）===\n",
    "    Xtr, Xva, ytr, yva = train_test_split(\n",
    "        Xtr_all, ytr_all, test_size=V[\"VAL_SIZE\"], stratify=ytr_all, random_state=seed\n",
    "    )\n",
    "\n",
    "    # === 归一化仅在训练子集拟合 ===\n",
    "    Xtr2, Xva2, scaler = minmax_scale_fit_transform(Xtr, Xva)\n",
    "    Xte2 = pd.DataFrame(scaler.transform(Xte), columns=Xte.columns)\n",
    "\n",
    "    # === 分层在训练子集上确定（互信息）===\n",
    "    feat_rank, mi_vals = rank_features_mi(Xtr2, ytr)   # ← 修正 ytr_sub 未定义\n",
    "    L1, L2, L3 = make_levels(feat_rank, V.get(\"LEVEL_PCTS\", [0.6,0.8,1.0]))\n",
    "\n",
    "    # === KWB 训练（训练子集）与概率（val/test）===\n",
    "    k = int(V[\"KWB_K\"])\n",
    "    kwb1 = KWBProbEstimator(k=k).fit(Xtr2[L1], ytr)\n",
    "    kwb2 = KWBProbEstimator(k=k).fit(Xtr2[L2], ytr)\n",
    "    kwb3 = KWBProbEstimator(k=k).fit(Xtr2[L3], ytr)\n",
    "    p1_va = kwb1.predict_proba(Xva2[L1]); p2_va = kwb2.predict_proba(Xva2[L2]); p3_va = kwb3.predict_proba(Xva2[L3])\n",
    "    p1_te = kwb1.predict_proba(Xte2[L1]); p2_te = kwb2.predict_proba(Xte2[L2]); p3_te = kwb3.predict_proba(Xte2[L3])\n",
    "\n",
    "    # === 验证集上 PSO 学阈值 ===\n",
    "    s3 = S3WDParams(\n",
    "        c1=V[\"S3_c1\"], c2=V[\"S3_c2\"], xi_min=V[\"S3_xi_min\"],\n",
    "        theta_pos=V[\"S3_theta_pos\"], theta_neg=V[\"S3_theta_neg\"],\n",
    "        penalty_large=V[\"S3_penalty_large\"]  # 若你的 S3WDParams 支持 gamma_last/gap，这里再加\n",
    "    )\n",
    "    pso = PSOParams(\n",
    "        particles=V[\"PSO_particles\"], iters=V[\"PSO_iters\"],\n",
    "        w_max=V[\"PSO_w_max\"], w_min=V[\"PSO_w_min\"],\n",
    "        c1=V[\"PSO_c1\"], c2=V[\"PSO_c2\"], seed=seed\n",
    "    )\n",
    "    (alphas, betas, gamma3), fit, detail = pso_learn_thresholds([p1_va, p2_va, p3_va], yva.values, s3, pso)\n",
    "\n",
    "    # === 测试集序贯三支决策 + 指标（MCC 传 y_true,y_pred；AUC 兜底）===\n",
    "    y_hat, flow = _seq_predict(p1_te, p2_te, p3_te, yte.values,\n",
    "                               float(alphas[0]), float(betas[0]),\n",
    "                               float(alphas[1]), float(betas[1]),\n",
    "                               float(gamma3))\n",
    "    mask = (y_hat >= 0)\n",
    "    yt, yp = yte[mask], y_hat[mask]\n",
    "    metrics = {\n",
    "        'F1':   f1_score(yt, yp),\n",
    "        'BAC':  balanced_accuracy_score(yt, yp),\n",
    "        'Prec': precision_score(yt, yp),\n",
    "        'Rec':  recall_score(yt, yp),\n",
    "        'MCC':  matthews_corrcoef(yt, yp),\n",
    "        'Kappa':cohen_kappa_score(yt, yp),\n",
    "        'AUC':  _safe_auc(yt, yp),\n",
    "    }\n",
    "    th = {'alpha1': float(alphas[0]), 'beta1': float(betas[0]),\n",
    "          'alpha2': float(alphas[1]), 'beta2': float(betas[1]), 'gamma3': float(gamma3)}\n",
    "    return metrics, flow, th, {'fit':float(fit), 'pen_bnd':detail.get('pen_bnd',0.0), 'pen_mono':detail.get('pen_mono',0.0)}\n",
    "\n",
    "# —— 跑 10 次 —— #\n",
    "rows = []\n",
    "base_seed = V.get(\"PSO_seed\", 42)\n",
    "for i in range(10):\n",
    "    m, flow, th, det = run_one_split(seed=base_seed + i)\n",
    "    rows.append({\n",
    "        **{k: (round(v,4) if isinstance(v, (int,float)) else v) for k,v in m.items()},\n",
    "        'L1_POS': flow['L1'][0], 'L1_BND': flow['L1'][1], 'L1_NEG': flow['L1'][2],\n",
    "        'L2_POS': flow['L2'][0], 'L2_BND': flow['L2'][1], 'L2_NEG': flow['L2'][2],\n",
    "        'L3_POS': flow['L3'][0], 'L3_NEG': flow['L3'][1],\n",
    "        **{k: round(v,4) for k,v in th.items()},\n",
    "        **{k: round(v,4) for k,v in det.items()}\n",
    "    })\n",
    "\n",
    "df_res = pd.DataFrame(rows)\n",
    "print(\"【10 次独立划分结果（前 5 行）】\")\n",
    "display(df_res.head())\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    'mean': df_res.mean(numeric_only=True).round(4),\n",
    "    'std':  df_res.std(numeric_only=True).round(4)\n",
    "})\n",
    "print(\"【均值 ± 标准差】\")\n",
    "display(summary.loc[['F1','BAC','Prec','Rec','MCC','Kappa','AUC']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719df993",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "\n",
    "summary.to_excel('../targets/credit.xlsx')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
